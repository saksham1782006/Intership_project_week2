{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7842b30b",
   "metadata": {},
   "source": [
    "to  predicts and optimizes building energy consumption to improve efficiency and reduce carbon emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9a417",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "130d2118",
   "metadata": {},
   "source": [
    "Code to preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b6c68",
   "metadata": {},
   "source": [
    "IMPORTING LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83c9667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75757, 64)\n",
      "(9705, 63)\n",
      "Columns to drop: ['direction_max_wind_speed', 'direction_peak_wind_speed', 'max_wind_speed', 'days_with_fog']\n",
      "Train shape after drop: (75757, 60)\n",
      "Test shape after drop: (9705, 59)\n",
      "State_Factor\n",
      "State_6     50840\n",
      "State_11     6412\n",
      "State_1      5618\n",
      "State_2      4871\n",
      "State_4      4300\n",
      "State_8      3701\n",
      "State_10       15\n",
      "Name: count, dtype: int64\n",
      "--------\n",
      "building_class\n",
      "Residential    43558\n",
      "Commercial     32199\n",
      "Name: count, dtype: int64\n",
      "--------\n",
      "facility_type\n",
      "Multifamily_Uncategorized                    39455\n",
      "Office_Uncategorized                         12512\n",
      "Education_Other_classroom                     3860\n",
      "Lodging_Hotel                                 2098\n",
      "2to4_Unit_Building                            1893\n",
      "Commercial_Other                              1744\n",
      "5plus_Unit_Building                           1273\n",
      "Warehouse_Nonrefrigerated                     1255\n",
      "Retail_Uncategorized                          1130\n",
      "Education_College_or_university               1056\n",
      "Nursing_Home                                   772\n",
      "Education_Uncategorized                        709\n",
      "Mixed_Use_Commercial_and_Residential           672\n",
      "Lodging_Dormitory_or_fraternity_sorority       669\n",
      "Warehouse_Distribution_or_Shipping_center      604\n",
      "Warehouse_Selfstorage                          577\n",
      "Grocery_store_or_food_market                   448\n",
      "Office_Medical_non_diagnostic                  447\n",
      "Health_Care_Inpatient                          409\n",
      "Religious_worship                              399\n",
      "Industrial                                     384\n",
      "Warehouse_Uncategorized                        330\n",
      "Mixed_Use_Predominantly_Commercial             256\n",
      "Parking_Garage                                 251\n",
      "Office_Bank_or_other_financial                 173\n",
      "Public_Assembly_Library                        160\n",
      "Public_Safety_Fire_or_police_station           157\n",
      "Public_Assembly_Other                          142\n",
      "Service_Vehicle_service_repair_shop            138\n",
      "Retail_Enclosed_mall                           120\n",
      "Retail_Strip_shopping_mall                     113\n",
      "Warehouse_Refrigerated                         113\n",
      "Public_Assembly_Entertainment_culture          110\n",
      "Education_Preschool_or_daycare                 108\n",
      "Laboratory                                     107\n",
      "Commercial_Unknown                              95\n",
      "Public_Assembly_Social_meeting                  88\n",
      "Lodging_Other                                   75\n",
      "Retail_Vehicle_dealership_showroom              74\n",
      "Public_Assembly_Recreation                      72\n",
      "Food_Sales                                      70\n",
      "Public_Assembly_Drama_theater                   70\n",
      "Service_Uncategorized                           68\n",
      "Food_Service_Restaurant_or_cafeteria            64\n",
      "Health_Care_Outpatient_Clinic                   56\n",
      "Health_Care_Uncategorized                       51\n",
      "Public_Safety_Uncategorized                     43\n",
      "Public_Assembly_Movie_Theater                   39\n",
      "Public_Safety_Courthouse                        38\n",
      "Public_Safety_Penitentiary                      37\n",
      "Health_Care_Outpatient_Uncategorized            36\n",
      "Data_Center                                     27\n",
      "Public_Assembly_Uncategorized                   25\n",
      "Office_Mixed_use                                18\n",
      "Food_Service_Uncategorized                      18\n",
      "Food_Service_Other                              17\n",
      "Mixed_Use_Predominantly_Residential              9\n",
      "Public_Assembly_Stadium                          9\n",
      "Service_Drycleaning_or_Laundry                   9\n",
      "Lodging_Uncategorized                            5\n",
      "Name: count, dtype: int64\n",
      "--------\n",
      "\n",
      "ðŸ“Š Random Forest with Preprocessing:\n",
      "MAE  : 19.6873\n",
      "RMSE : 39.6074\n",
      "RÂ²   : 0.5419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pipeline.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#choosen imp lib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#read file\n",
    "test=pd.read_csv(\"widsdatathon2022/test.csv\")\n",
    "train=pd.read_csv(\"widsdatathon2022/train.csv\")\n",
    "#check shape\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "#check data types\n",
    "test.isnull().sum()/test.shape[0]*100\n",
    "train.isnull().sum()/train.shape[0]*100\n",
    "missing_percent = train.isnull().sum() * 100 / train.shape[0]\n",
    "\n",
    "# columns to drop\n",
    "cols_to_drop = missing_percent[missing_percent > 50].index\n",
    "print(\"Columns to drop:\", cols_to_drop.tolist())\n",
    "train = train.drop(columns=cols_to_drop)\n",
    "test = test.drop(columns=cols_to_drop)\n",
    "print(\"Train shape after drop:\", train.shape)\n",
    "print(\"Test shape after drop:\", test.shape)\n",
    "train.duplicated().sum()\n",
    "test.duplicated().sum()\n",
    "\n",
    "for i in train.select_dtypes(include=\"object\").columns:\n",
    "    print(train[i].value_counts())\n",
    "    print(\"--------\")\n",
    "\n",
    "train.describe().T\n",
    "test.describe().T\n",
    "#check for unique values\n",
    "\"\"\"for i in train.select_dtypes(include=\"number\").columns:\n",
    "    sns.boxplot(data=train,x=i)\n",
    "    plt.show()\"\"\"\n",
    "train.select_dtypes(include=\"number\").columns\n",
    "\"\"\"for i in ['Year_Factor', 'floor_area', 'year_built', 'energy_star_rating',\n",
    "       'ELEVATION', 'january_min_temp', 'january_avg_temp', 'january_max_temp',\n",
    "       'february_min_temp', 'february_avg_temp', 'february_max_temp',\n",
    "       'march_min_temp', 'march_avg_temp', 'march_max_temp', 'april_min_temp',\n",
    "       'april_avg_temp', 'april_max_temp', 'may_min_temp', 'may_avg_temp',\n",
    "       'may_max_temp', 'june_min_temp', 'june_avg_temp', 'june_max_temp',\n",
    "       'july_min_temp', 'july_avg_temp', 'july_max_temp', 'august_min_temp',\n",
    "       'august_avg_temp', 'august_max_temp', 'september_min_temp',\n",
    "       'september_avg_temp', 'september_max_temp', 'october_min_temp',\n",
    "       'october_avg_temp', 'october_max_temp', 'november_min_temp',\n",
    "       'november_avg_temp', 'november_max_temp', 'december_min_temp',\n",
    "       'december_avg_temp', 'december_max_temp', 'cooling_degree_days',\n",
    "       'heating_degree_days', 'precipitation_inches', 'snowfall_inches',\n",
    "       'snowdepth_inches', 'avg_temp', 'days_below_30F', 'days_below_20F',\n",
    "       'days_below_10F', 'days_below_0F', 'days_above_80F', 'days_above_90F',\n",
    "       'days_above_100F', 'days_above_110F','id']:\n",
    "    sns.scatterplot(data=train,x=i,y=\"site_eui\")\n",
    "    plt.show()\n",
    "s=train.select_dtypes(include=\"number\").corr()\n",
    "plt.figure(figsize=(40,40))\n",
    "sns.heatmap(s,annot=True)\"\"\"\n",
    "train.isnull().sum()\n",
    "for i in[\"year_built\",\"energy_star_rating\"]:\n",
    "    train[i].fillna(train[i].median(),inplace=True)\n",
    "train.isnull().sum()\n",
    "def wisker(col):\n",
    "     q1,q3=np.percentile(col,[25,75])\n",
    "     iqr=q3-q1\n",
    "     lower_bound=q1-1.5*iqr\n",
    "     upper_bound=q3+1.5*iqr\n",
    "     return lower_bound,upper_bound\n",
    "for i in ['ELEVATION', 'january_min_temp', 'january_avg_temp', 'january_max_temp',\n",
    "       'february_min_temp', 'february_avg_temp', 'february_max_temp',\n",
    "       'march_min_temp', 'march_avg_temp', 'march_max_temp', 'april_min_temp',\n",
    "       'april_avg_temp', 'april_max_temp', 'may_min_temp', 'may_avg_temp',\n",
    "       'may_max_temp', 'june_min_temp', 'june_avg_temp', 'june_max_temp',\n",
    "       'july_min_temp', 'july_avg_temp', 'july_max_temp', 'august_min_temp',\n",
    "       'august_avg_temp', 'august_max_temp', 'september_min_temp',\n",
    "       'september_avg_temp', 'september_max_temp', 'october_min_temp',\n",
    "       'october_avg_temp', 'october_max_temp', 'november_min_temp',\n",
    "       'november_avg_temp', 'november_max_temp', 'december_min_temp',\n",
    "       'december_avg_temp', 'december_max_temp','precipitation_inches', 'snowfall_inches',\n",
    "       'snowdepth_inches', 'avg_temp', 'days_below_30F', 'days_below_20F',\n",
    "       'days_below_10F', 'days_below_0F', 'days_above_80F', 'days_above_90F',\n",
    "       'days_above_100F', 'days_above_110F','year_built']:\n",
    "    lower,upper=wisker(train[i])\n",
    "    train[i]=np.where(train[i]>upper,upper,train[i])\n",
    "    train[i]=np.where(train[i]<lower,lower,train[i])\n",
    "\"\"\"for i in ['ELEVATION', 'january_min_temp', 'january_avg_temp', 'january_max_temp',\n",
    "       'february_min_temp', 'february_avg_temp', 'february_max_temp',\n",
    "       'march_min_temp', 'march_avg_temp', 'march_max_temp', 'april_min_temp',\n",
    "       'april_avg_temp', 'april_max_temp', 'may_min_temp', 'may_avg_temp',\n",
    "       'may_max_temp', 'june_min_temp', 'june_avg_temp', 'june_max_temp',\n",
    "       'july_min_temp', 'july_avg_temp', 'july_max_temp', 'august_min_temp',\n",
    "       'august_avg_temp', 'august_max_temp', 'september_min_temp',\n",
    "       'september_avg_temp', 'september_max_temp', 'october_min_temp',\n",
    "       'october_avg_temp', 'october_max_temp', 'november_min_temp',\n",
    "       'november_avg_temp', 'november_max_temp', 'december_min_temp',\n",
    "       'december_avg_temp', 'december_max_temp','precipitation_inches', 'snowfall_inches',\n",
    "       'snowdepth_inches', 'avg_temp', 'days_below_30F', 'days_below_20F',\n",
    "       'days_below_10F', 'days_below_0F', 'days_above_80F', 'days_above_90F',\n",
    "       'days_above_100F', 'days_above_110F','year_built']:\n",
    "    sns.boxplot(train[i])\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Scale numerical features\n",
    "\"\"\"from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# -----------------------------\n",
    "# Train-validation split\n",
    "# -----------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Define features (X) and target (y)\n",
    "Xx= train.drop(columns=['site_eui', 'id'])   # drop target + id\n",
    "y = train['site_eui']                        # target variable\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Define ML models\n",
    "# -----------------------------\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=200, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"XGBoost\": xgb.XGBRegressor(\n",
    "        n_estimators=300, learning_rate=0.1, max_depth=6, random_state=42\n",
    "    )\n",
    "\n",
    "}\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score \n",
    "results = {} \n",
    "for name, model in models.items():\n",
    "     model.fit(X_train, y_train) \n",
    "     y_pred = model.predict(X_val) \n",
    "     mae = mean_absolute_error(y_val, y_pred) \n",
    "     mse = mean_squared_error(y_val, y_pred) # Mean Squared Error Mean Squared Error \n",
    "     r2 = r2_score(y_val, y_pred) \n",
    "results[name] = {\"MAE\": mae, \"RMSE\": mse, \"RÂ²\": r2} # Convert results to dataframe for easy comparison \n",
    "results_df = pd.DataFrame(results).T \n",
    "print(\"\\nðŸ“Š Model Comparison:\\n\") \n",
    "print(results_df.sort_values(by=\"RMSE\"))\"\"\"\n",
    "#Training part of ml \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# -----------------------------\n",
    "# Features & Target\n",
    "# -----------------------------\n",
    "X = train.drop(columns=[\"site_eui\"])   # all features\n",
    "y = train[\"site_eui\"]                  # target\n",
    "\n",
    "# -----------------------------\n",
    "# Separate numeric & categorical\n",
    "# -----------------------------\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# -----------------------------\n",
    "# Preprocessing\n",
    "# -----------------------------\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Pipeline: Preprocessor + Model\n",
    "# -----------------------------\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# Train-test split\n",
    "# -----------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# -----------------------------\n",
    "# Train\n",
    "# -----------------------------\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# Predict & Evaluate\n",
    "# -----------------------------\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "rmse = mean_squared_error(y_val, y_pred) ** 0.5\n",
    "\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"\\nðŸ“Š Random Forest with Preprocessing:\")\n",
    "print(f\"MAE  : {mae:.4f}\")\n",
    "print(f\"RMSE : {rmse:.4f}\")\n",
    "print(f\"RÂ²   : {r2:.4f}\")\n",
    "import joblib\n",
    "\n",
    "\n",
    "joblib.dump(pipeline, \"pipeline.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2561105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
